# Activation Functions
The `activations` module implements several common activation functions:

- Rectified linear units (ReLU)
- Leaky rectified linear units
  ([Maas, Hannun, & Ng, 2013](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf))
- Hyperbolic tangent (tanh)
- Logistic sigmoid
- Affine
- Softmax

## Plots
<p align="center">
<img src="img/plot.png" align='center' height="550" />
</p>
